\chapter{Discussion and Future Work}
This chapter will evaluate and discuss the results presented in the previous chapter. A comparison will be drawn to other deep learning projects and improvements suggested for the next iteration. The final section of the chapter will outline the work that needs to be done following on from this to implement a successful and highly accurate CNN.
\section{Model Accuracy}
The highest accuracy recorded for any combination of hyperparameters was 55\%. As there were only two possible classes, this result is marginally better than random chance, which would have achieved an accuracy of 50\%. The CNN created here would not be fit for any image classification procedure at this level of accuracy. However, the primary objective of this project was to produce the necessary workflow to implement a successful CNN specifically for remote sensing. The framework now exists, but it is necessary to continue tuning the hyperparameters to find the optimal combination to reduce processing times and increase accuracy. 
\par
The main reason why the accuracy of the network proposed here is poor is that the size of the training dataset is small. The CNN produced by \citet{krizhevsky17}, achieved accuracy's of over 83\% but the dataset they used had over 1.2million training images, 50,000 validation images and 150,000 testing images. The CNN here used 108 training images, 36 validation images and 36 testing images. It was assumed that by taking a random crop of each of the training samples over each epoch, the size of the training set would by artificially increased as the network would see a different part of each image, each epoch. However, this technique is still limited by the maximum number of training samples from which to take random crops. Perhaps a more optimal way would be to split the large 5000x5000 pix images into smaller 1000x1000 pix tiles, this would increase the size of the training dataset by a factor of 5. 
\par
Another way to synthesise a larger dataset is to train over a longer period of time. Once a combination of hyperparameters that produce an acceptable accuracy over 100 epochs has been selected, the network should be trained for an extended period in this state.  

\section{Other Hyperparameters}
Several hyperparameters were fixed for this project, these include the optimiser function; the number of Conv layers; the spatial dimensions of the input image; the amount of zero-padding, the stride, and the size of the kernels for the Conv filters. When all of these are adjusted they affect the dimensions at the lowest resolution. 
\par
The input dimensions for the initial image was fixed at 128x128 pix. These dimensions, in combination with the U-Net architecture, resulted in a final Conv dimension of 8x8 pix, with the depth dimension corresponding to the number of filters in the previous layer. By changing the initial spatial dimension and the number of Conv layers, the final dimensions of the Conv layer could be fine-tuned and tested to find the most optimal combination. Adjusting the size of the kernel, the stride and the amount of zero padding would also affect the spatial dimensions of the data between each Conv layer and would affect the spatial dimensions in the final layer. It is important to keep a note of how adjusting the kernel size, stride and padding will transform the input as it enters each Conv layer.
\par
These hyperparameters were fixed for this study because they give the optimal combination while not using any pooling layers to aggregate the data. As mentioned in the previous chapters, this study aims to create an architecture that can generalise well to other areas. By choosing to fix these parameters, the study hopes to have a implemented a successful CNN that is less resource intensive. 
\section{Successful CNNs}
There are many recent examples of CNNs that have been successfully used for image classification problems (\cite{audebert18,krizhevsky17, long15, maggiori17a, marmanis16, mnih13, shelhamer17, volpi17}). The architecture proposed by \citet{krizhevsky17} is the most successful and is trained using a subset of the ImageNet dataset as mentioned previously. These large training datasets are limited in their use, for example, they would not be able to be used to classify landslides or flooding after they had occurred as these are not images that are included in the datasets. This study has created a workflow so that the user can implement their own CNN using a relatively small dataset that is then fine-tuned to classify other regions successfully. While a building dataset was used for this study, it has been designed so that any dataset could be used and implemented with relative ease.

\section{Future Work}
The next step for this process is to automate the hyperparameter searching process as this is where most resources are consumed. The technique used here was a simple grid search where each model uses sequentially different combinations of hyperparameters. A small improvement on this would be to use a random search process. It has been shown that randomly using hyperparameters within a specific range is easier to carry out than grid experiments. Grid searching allocates too many trials to the exploration of dimensions that do not matter (\cite{bergstra12}). 
\par
A technique that has been gaining traction in recent years is bayesian hyperparameter optimisation to find the best combination for the dataset. The user needs to define the hyperparameter domain in which to search and then creates a function that can be used to evaluate the success of the model in a single score. Bayesian optimisation constructs a probabilistic model for our function and exploits the model to decide where in our domain to next evaluate the function using all the information from previous iterations (\cite{snoek12}).