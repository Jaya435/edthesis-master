\chapter{Results}
After extensive testing of different hyperparameters, the model that produced the best accuracy was used to predict the class of each pixel for the data in the test dataset that was left untouched during training. 
\section{Hyperparameters}
The following sections will show the effect of changing one of the hyperparameters while fixing the other two. The key four characteristics to observe as mentioned in Section \ref{sec.model_training} are:
\begin{enumerate}
    \item The loss should be decreasing, rapidly at first and then slowing down as training progresses. Once the decrease in loss stagnates, training should stop.
    \item Validation accuracy should be slightly higher than training accuracy once training is complete. Training should continue while validation accuracy is lower than the training accuracy.
    \item If training accuracy reduces without an increase in validation accuracy, training should continue
    \item Once validation accuracy starts to increase, training should discontinue. 
\end{enumerate}
\subsection{Adjusting the Learning Rate}
Adjusting the learning rate adjusts the length the weight vector is travelled on before the loss is calculated again for the next iteration. A smaller learning rate means a smaller adjustment to the weights between iterations. In Figure \ref{fig.lr_varied} the loss rate that shows the most optimal rate of decay is a learning rate of 0.001. There is a decreasing loss rate over time and it does not appear to have reduced in gradient by the end of 100 epochs. Validation accuracy is still lower than training accuracy and so training should continue. In contrast, the loss for a learning rate of 0.0001 shows large variation in the loss and is not displaying a fast decay. The other two learning rate show a loss that has converged and is no longer decreasing so would not be suitable for continued training.
\begin{figure}[htpb]
    \centering
    \includegraphics[width=\textwidth]{\dir/figs/lr_varied.png}
    \caption[Example of the affect of changing the learning rate]{Example of how the learning rate affects the training progression over 100 epochs. Learning rate increases from  0.0001 to 0.01 by a factor of ten in each plot. The accuracy values have been smoothed using a moving window of 5, the smoothed training loss uses a moving window of 10.}
    \label{fig.lr_varied}
\end{figure}
\subsection{Adjusting the Batch Size}
As demonstrated in Section \ref{sec.batch_size}, changing the batch size affects the depth of the stack of training samples that input into the network on each iteration. Where the batch size is larger than the number of training samples, the batch dimension becomes the number of training samples. For the final plot, the batch size will be 108. All the plots display a decreasing loss over 100 epochs, changing the size of the batch for this set of hyperparameters has not effected the variation in loss as loss is calculated as an average for the batch. Variation in accuracy, however, decreases as the size of the batch increases. None of the plots shows better or worse learning rate curves and accuracy so would all be explored again in the next iteration. 
\begin{figure}[htpb]
    \centering
    \includegraphics[width=\textwidth]{\dir/figs/batch_varied.png}
    \caption[Example of the affect of changing the batch size]{Example of how the batch size affects the training progression over 100 epochs. The number of filters and the learning rate remain fixed, while the batch size is increased by a factor of two for each of the plots. Accuracy has been smoothed using a moving window of five and plotting the average, the smoothed training loss uses a moving window of 10.}
    \label{fig.batch_varied}
\end{figure}
\subsection{Adjusting the Number of Filters}
Adjusting the number of filters adjusts how many features are being extracted from the input image and fed into the next layer. With all of the plots, there is a gradual reduction in the loss as training progresses. Both an initial filter of two and four show loss rates that decrease with the greatest accuracy, so further iterations would explore other hyperparameters in that domain.
\begin{figure}[htpb]
    \centering
    \includegraphics[width=\textwidth]{\dir/figs/net_varied.png}
    \caption[Example of the affect of changing the number of filters in each Convolution layer]{Example of how changing the number of filters affects the learning rate. The plots show the number of filters increasing from 2 to 128 by a factor of 2 each time. Accuracy has been smoothed using a moving window of five and plotting the average, the smoothed training loss uses a moving window of 10.}
    \label{fig.net_varied}
\end{figure}
\section{Accuracy}
Throughout the training process, the network saves the state of the weights and the biases whenever the loss rate reduces. This model state can then be loaded again at a later data and training can continue. The final stage of training takes the model state that results in the lowest loss and applies it to the test data to make a prediction and evaluate the accuracy.
\par
The most accurate model is the model that will be used to make predictions on the test dataset. Table \ref{tab.hyper_param}, shows the results of training over 100 epochs for the defined combinations of the three hyperparameters. A large number of the results are not accurate, with their accuracy below 10\%. The highest accuracy produced is 44-55\%, each of these model runs had 32 initial filters and the final two had batch sizes of 16. The most accurate model had a learning rate of 0.01 and the second most accurate had a learning rate of 0.001. 
\begin{table}[htpb]
\centering
\begin{adjustbox}{totalheight=\textheight-2\baselineskip}
\begin{tabular}{rrrrr}
\toprule
 Accuracy &   Time &  Batch Size &  Learning Rate &  Filter Size \\
\midrule
    0.000 &   5.62 &        64.0 &         0.1000 &         64.0 \\
    0.000 &   5.23 &       128.0 &         0.0100 &         16.0 \\
    0.057 &   5.33 &       128.0 &         0.1000 &         16.0 \\
    0.076 &   5.11 &        32.0 &         0.1000 &         32.0 \\
    0.243 &   5.71 &       128.0 &         0.0010 &         32.0 \\
    0.401 &   7.37 &        32.0 &         0.1000 &         16.0 \\
    0.471 &   5.06 &        64.0 &         0.1000 &         16.0 \\
    0.641 &   4.92 &        64.0 &         0.1000 &         32.0 \\
    1.398 &   6.02 &        32.0 &         0.0010 &         64.0 \\
    1.886 &   5.98 &       128.0 &         0.0100 &         64.0 \\
    2.203 &   6.03 &       128.0 &         0.1000 &         64.0 \\
    2.243 &   4.82 &       256.0 &         0.0100 &         64.0 \\
    2.258 &  10.50 &        64.0 &         0.0100 &         64.0 \\
    2.754 &   5.75 &        64.0 &         0.0010 &         64.0 \\
    3.100 &   6.32 &        64.0 &         0.0001 &         32.0 \\
    3.892 &   5.83 &       128.0 &         0.0001 &         16.0 \\
    3.904 &   5.34 &        32.0 &         0.0010 &         16.0 \\
    3.937 &   5.24 &        32.0 &         0.0100 &         64.0 \\
    4.035 &   5.06 &       128.0 &         0.0010 &         16.0 \\
    4.343 &   5.30 &        64.0 &         0.0001 &          8.0 \\
    4.548 &   5.48 &       256.0 &         0.1000 &         16.0 \\
    4.584 &   5.71 &        32.0 &         0.0100 &         16.0 \\
    5.326 &   4.85 &       256.0 &         0.1000 &         64.0 \\
    5.352 &   7.45 &       256.0 &         0.1000 &         32.0 \\
    5.995 &   5.49 &       256.0 &         0.0010 &         64.0 \\
    7.756 &   5.63 &        32.0 &         0.0001 &         64.0 \\
    7.840 &   5.92 &        16.0 &         0.1000 &         16.0 \\
    8.232 &   5.36 &        32.0 &         0.0001 &          8.0 \\
    8.282 &   4.81 &       256.0 &         0.0100 &         16.0 \\
    8.585 &   7.60 &       128.0 &         0.0100 &         32.0 \\
    8.867 &   5.29 &       256.0 &         0.0001 &         64.0 \\
    9.668 &   5.46 &        64.0 &         0.0001 &         64.0 \\
    9.826 &   5.99 &       256.0 &         0.0010 &         32.0 \\
   10.535 &   5.67 &        16.0 &         0.0001 &         32.0 \\
   10.712 &   6.65 &        32.0 &         0.0010 &         32.0 \\
   10.836 &   5.94 &        16.0 &         0.1000 &         64.0 \\
   10.853 &   5.52 &        16.0 &         0.0010 &         16.0 \\
   11.248 &   5.87 &        16.0 &         0.0100 &         64.0 \\
   12.138 &   5.22 &        32.0 &         0.1000 &         64.0 \\
   12.249 &   5.76 &       256.0 &         0.0100 &         32.0 \\
   12.302 &   5.06 &       256.0 &         0.0001 &         16.0 \\
   13.822 &   5.96 &        64.0 &         0.0100 &         16.0 \\
   14.965 &   5.97 &        16.0 &         0.1000 &         32.0 \\
   15.332 &   5.58 &        64.0 &         0.0001 &         16.0 \\
   15.789 &   5.24 &       256.0 &         0.0001 &          8.0 \\
   16.824 &   4.81 &        32.0 &         0.0001 &         16.0 \\
   17.999 &   5.52 &       128.0 &         0.0001 &          8.0 \\
   18.578 &   4.91 &       128.0 &         0.0001 &         64.0 \\
   18.845 &   4.88 &        64.0 &         0.0010 &         32.0 \\
   19.220 &   5.21 &       256.0 &         0.0010 &         16.0 \\
   19.585 &   5.35 &        16.0 &         0.0100 &         16.0 \\
   21.727 &   5.07 &       256.0 &         0.0001 &         32.0 \\
   22.014 &   5.06 &        32.0 &         0.0001 &         32.0 \\
   22.790 &   5.53 &       128.0 &         0.0001 &         32.0 \\
   23.090 &   5.22 &       128.0 &         0.0010 &         64.0 \\
   24.361 &   5.59 &        16.0 &         0.0010 &         64.0 \\
   26.538 &   4.74 &        16.0 &         0.0001 &         16.0 \\
   26.827 &   4.99 &        16.0 &         0.0001 &         64.0 \\
   37.611 &   5.16 &        64.0 &         0.0010 &         16.0 \\
   44.251 &   7.37 &       128.0 &         0.1000 &         32.0 \\
   45.031 &   5.16 &        16.0 &         0.0010 &         32.0 \\
   55.069 &   5.81 &        16.0 &         0.0100 &         32.0 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption[Hyperparameter Search Results]{Results from hyperparameter searching to find the optimal hyperparameters with which to train the network. Accuracy is increasing with each change in model hyperparameters.}
\label{tab.hyper_param}
\end{table}
Figure \ref{fig.3d_hyperparams} shows how most hyperparameter combinations produce low accuracy. A large number of filters does not increase model accuracy. However, it is difficult to say clearly which other hyperparameter values worked well and which didn't.  
\begin{figure}[htpb]
    \centering
    \includegraphics[width=\textwidth]{\dir/figs/3d_params.png}
    \caption[3D representation of the accuracy of different hyperparameter combinations]{A 3D representation of Table \ref{tab.hyper_param} plotting the three hyperparameters and colouring them according to their accuracy. Points that are bluer in colour represent ones that produce an accuracy approaching 50\%.  }
    \label{fig.3d_hyperparams}
\end{figure}
\section{Predictions}
To see how well the model has predicted where buildings are or aren't, the weights and biases of the network are fixed and all the images are sequentially passed through the network and a prediction made on a pixel by pixel basis. Figure \ref{fig.prediction} shows the result of the prediction on the same images as exampled in Figure \ref{fig.inria_dataset}. 

\begin{figure}[htpb]
    \centering
    \includegraphics[height=\dimexpr \textheight - 4\baselineskip\relax]{\dir/figs/Predicted_Grid.png}
    \caption[Image Prediction using the model with the best accuracy]{Image prediction using the model trained with the hyperparameters that produce the highest accuracy. Side by side comparison of the ground truth labels and the model prediction is clearly shown.}
    \label{fig.prediction}
\end{figure}